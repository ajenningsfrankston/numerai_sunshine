{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T04:20:47.568760Z",
     "iopub.status.busy": "2023-03-27T04:20:47.568364Z",
     "iopub.status.idle": "2023-03-27T04:20:58.158194Z",
     "shell.execute_reply": "2023-03-27T04:20:58.157421Z",
     "shell.execute_reply.started": "2023-03-27T04:20:47.568731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.65.0\n",
      "Collecting halo\n",
      "  Using cached halo-0.0.31-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from halo) (1.16.0)\n",
      "Collecting log-symbols>=0.0.14\n",
      "  Using cached log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
      "Collecting colorama>=0.3.9\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting spinners>=0.0.24\n",
      "  Using cached spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
      "Installing collected packages: spinners, termcolor, colorama, log-symbols, halo\n",
      "Successfully installed colorama-0.4.6 halo-0.0.31 log-symbols-0.0.14 spinners-0.0.24 termcolor-2.2.0\n",
      "Collecting numerapi\n",
      "  Using cached numerapi-2.13.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: tqdm>=4.29.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from numerapi) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from numerapi) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from numerapi) (1.4.2)\n",
      "Requirement already satisfied: pytz in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from numerapi) (2022.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from numerapi) (8.1.3)\n",
      "Requirement already satisfied: requests in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from numerapi) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pandas>=1.1.0->numerapi) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from python-dateutil->numerapi) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests->numerapi) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests->numerapi) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests->numerapi) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from requests->numerapi) (2.0.12)\n",
      "Installing collected packages: numerapi\n",
      "Successfully installed numerapi-2.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install halo\n",
    "!pip install numerapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-27T04:22:32.341739Z",
     "iopub.status.busy": "2023-03-27T04:22:32.341326Z",
     "iopub.status.idle": "2023-03-27T04:22:32.709591Z",
     "shell.execute_reply": "2023-03-27T04:22:32.708847Z",
     "shell.execute_reply.started": "2023-03-27T04:22:32.341712Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset files...\n",
      "\r"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL,\n",
    ")\n",
    "\n",
    "correlation_threshold = 0.005\n",
    "\n",
    "# download all the things\n",
    "\n",
    "napi = NumerAPI()\n",
    "\n",
    "current_round = napi.get_current_round()\n",
    "\n",
    "# Tournament data changes every week, so we specify the round in their name. Training\n",
    "# and validation data only change periodically, so no need to download them every time.\n",
    "print(\"Downloading dataset files...\")\n",
    "dataset_name = \"v4.1\"\n",
    "feature_set_name = \"correlation_threshold\"\n",
    "\n",
    "Path(f\"./{dataset_name}\").mkdir(parents=False, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-27T04:22:46.960956Z",
     "iopub.status.busy": "2023-03-27T04:22:46.960342Z",
     "iopub.status.idle": "2023-03-27T04:23:00.519302Z",
     "shell.execute_reply": "2023-03-27T04:23:00.518645Z",
     "shell.execute_reply.started": "2023-03-27T04:22:46.960919Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 04:22:47,638 INFO numerapi.utils: target file already exists\n",
      "2023-03-27 04:22:47,639 INFO numerapi.utils: download complete\n",
      "2023-03-27 04:22:48,158 INFO numerapi.utils: target file already exists\n",
      "2023-03-27 04:22:48,159 INFO numerapi.utils: download complete\n",
      "2023-03-27 04:22:48,631 INFO numerapi.utils: target file already exists\n",
      "2023-03-27 04:22:48,632 INFO numerapi.utils: download complete\n",
      "2023-03-27 04:22:49,100 INFO numerapi.utils: target file already exists\n",
      "2023-03-27 04:22:49,101 INFO numerapi.utils: download complete\n",
      "2023-03-27 04:22:49,497 INFO numerapi.utils: target file already exists\n",
      "2023-03-27 04:22:49,498 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2386"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# we'll use the int8 in this example in order to save RAM.\n",
    "# if you remove the int8 suffix for each of these files, you'll get features between 0 and 1 as floats.\n",
    "# int_8 files are much smaller...\n",
    "# but are harder to work with because some packages don't like ints and the way NAs are encoded.\n",
    "\n",
    "# napi.download_dataset(f\"{dataset_name}/train.parquet\")\n",
    "# napi.download_dataset(f\"{dataset_name}/validation.parquet\")\n",
    "# napi.download_dataset(f\"{dataset_name}/live.parquet\", f\"{dataset_name}/live_{current_round}.parquet\")\n",
    "\n",
    "napi.download_dataset(f\"{dataset_name}/train_int8.parquet\")\n",
    "napi.download_dataset(f\"{dataset_name}/validation_int8.parquet\")\n",
    "napi.download_dataset(\n",
    "    f\"{dataset_name}/live_int8.parquet\",\n",
    "    f\"{dataset_name}/live_int8_{current_round}.parquet\",\n",
    ")\n",
    "\n",
    "napi.download_dataset(f\"{dataset_name}/validation_example_preds.parquet\")\n",
    "napi.download_dataset(f\"{dataset_name}/features.json\")\n",
    "\n",
    "print(\"Reading minimal training data\")\n",
    "# read the feature metadata and get a feature set (or all the features)\n",
    "\n",
    "\n",
    "with open(f\"{dataset_name}/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "# include features above a correlation threshold\n",
    "\n",
    "df = pd.DataFrame(feature_metadata[\"feature_stats\"])\n",
    "df = df.T\n",
    "df['spearman_corr_w_target_nomi_20_mean'] = df['spearman_corr_w_target_nomi_20_mean'].abs()\n",
    "dfs = df[(df['spearman_corr_w_target_nomi_20_mean'] > correlation_threshold)]\n",
    "features = list(dfs.index)\n",
    "\n",
    "\n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "# read in just those features along with era and target columns\n",
    "read_columns = features + target_cols + [ERA_COL, DATA_TYPE_COL]\n",
    "\n",
    "# note: sometimes when trying to read the downloaded data you get an error about invalid magic parquet bytes...\n",
    "# if so, delete the file and rerun the napi.download_dataset to fix the corrupted file\n",
    "training_data = pd.read_parquet(\n",
    "    f\"{dataset_name}/train_int8.parquet\", columns=read_columns\n",
    ")\n",
    "validation_data = pd.read_parquet(\n",
    "    f\"{dataset_name}/validation_int8.parquet\", columns=read_columns\n",
    ")\n",
    "live_data = pd.read_parquet(f\"{dataset_name}/live_int8_{current_round}.parquet\", columns=read_columns)\n",
    "\n",
    "# reduce the number of eras to every 4th era to speed things up... uncomment these lines to speed things up.\n",
    "# every_4th_era = training_data[ERA_COL].unique()[::4]\n",
    "# training_data = training_data[training_data[ERA_COL].isin(every_4th_era)]\n",
    "# every_4th_era = validation_data[ERA_COL].unique()[::4]\n",
    "# validation_data = validation_data[validation_data[ERA_COL].isin(every_4th_era)]\n",
    "\n",
    "\n",
    "# get all the data to possibly use for training\n",
    "# all_data = pd.concat([training_data, validation_data]) - we don't do this\n",
    "all_data = training_data\n",
    "\n",
    "# save indices for easier data selection later\n",
    "training_index = training_data.index\n",
    "validation_index = validation_data.index\n",
    "all_index = all_data.index\n",
    "\n",
    "# delete training and validation data to save space\n",
    "del training_data\n",
    "del validation_data\n",
    "gc.collect()  # clear up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-27T04:23:07.101893Z",
     "iopub.status.busy": "2023-03-27T04:23:07.101289Z",
     "iopub.status.idle": "2023-03-27T04:23:32.093800Z",
     "shell.execute_reply": "2023-03-27T04:23:32.092703Z",
     "shell.execute_reply.started": "2023-03-27T04:23:07.101865Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up NAs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_ridge_v4.1_correlation_threshold_target_nomi_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['n000101811a8a843', 'n001e1318d5072ac', 'n002a9c5ab785cbb',\\n       'n002ccf6d0e8c5ad', 'n0051ab821295c29', 'n008361ac9e9bd47',\\n       'n009e95486e1d64c', 'n00b093a02b84295', 'n00b84e81c983a0a',\\n       'n00d453202699c32',\\n       ...\\n       'nff0ba25730cb4f4', 'nff14f3a7a34a805', 'nff19fe36ef2b78e',\\n       'nff3182cfedd3ed2', 'nff4ccde16b61d5c', 'nff7a622fe031230',\\n       'nff7ab4bfe012ac2', 'nff8fb80c65a1d33', 'nff8ff013358e5a5',\\n       'nff955a9f9829e9c'],\\n      dtype='object', name='id', length=2456749)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m     save_model(train_model, train_data_model_name)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# predict on validation data\u001b[39;00m\n\u001b[1;32m     49\u001b[0m all_data\u001b[38;5;241m.\u001b[39mloc[validation_index, prediction_col] \u001b[38;5;241m=\u001b[39m train_model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mall_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalidation_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# do the same thing for all data (for predicting on live)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexing.py:961\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexing.py:1147\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# ugly hack for GH #836\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexing.py:1098\u001b[0m, in \u001b[0;36m_LocIndexer._multi_take\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1099\u001b[0m     axis: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[1;32m   1101\u001b[0m }\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexing.py:1099\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m-> 1099\u001b[0m     axis: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[1;32m   1101\u001b[0m }\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexing.py:1327\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1324\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1325\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1327\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['n000101811a8a843', 'n001e1318d5072ac', 'n002a9c5ab785cbb',\\n       'n002ccf6d0e8c5ad', 'n0051ab821295c29', 'n008361ac9e9bd47',\\n       'n009e95486e1d64c', 'n00b093a02b84295', 'n00b84e81c983a0a',\\n       'n00d453202699c32',\\n       ...\\n       'nff0ba25730cb4f4', 'nff14f3a7a34a805', 'nff19fe36ef2b78e',\\n       'nff3182cfedd3ed2', 'nff4ccde16b61d5c', 'nff7a622fe031230',\\n       'nff7ab4bfe012ac2', 'nff8fb80c65a1d33', 'nff8ff013358e5a5',\\n       'nff955a9f9829e9c'],\\n      dtype='object', name='id', length=2456749)] are in the [index]\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Int8 datatype has pd.NA which don't play nice with models.  We simply fill NA with median values here\n",
    "print(\"cleaning up NAs\")\n",
    "all_data[features] = all_data[features].fillna(all_data[features].median(skipna=True))\n",
    "all_data[features] = all_data[features].astype(\"int8\")  # make sure change to float32 if using the non int8 data!\n",
    "live_data[features] = live_data[features].fillna(\n",
    "    all_data[features].median(skipna=True)\n",
    ")  # since live data is only one era, we need to use the median for all eras\n",
    "live_data[features] = live_data[features].astype(\"int8\")  # make sure change to float32 if using the non int8 data!\n",
    "# Alternatively could convert nan columns to be floats and replace pd.NA with np.nan\n",
    "\n",
    "\n",
    "# recommended params\n",
    "params_name = \"ridge\"\n",
    "\n",
    "\n",
    "# loop through all of our favorite targets and build models on each of them -\n",
    "# one over training data, one over all available data\n",
    "# for the train_data models, we'll then predict on validation data\n",
    "# for the all_data models, we'll predict on live\n",
    "targets = [\n",
    "    \"target_nomi_v4_20\",\n",
    "    \"target_jerome_v4_60\",\n",
    "    \"target_ralph_v4_20\",\n",
    "    \"target_tyler_v4_20\",\n",
    "    \"target_victor_v4_20\",\n",
    "    \"target_waldo_v4_20\",\n",
    "]\n",
    "prediction_cols = []\n",
    "for target in tqdm(targets):\n",
    "    prediction_col = f\"{params_name}_{dataset_name}_{feature_set_name}_{target}\"\n",
    "    train_data_model_name = f\"train_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{train_data_model_name}'\")\n",
    "    train_model = load_model(train_data_model_name)\n",
    "    if not train_model:\n",
    "        print(f\"model not found, creating new one\")\n",
    "        train_model = Ridge(alpha=0.9)\n",
    "        # train on all training data and save the model, so we don't have to train next time\n",
    "        target_train_index = (\n",
    "            all_data.loc[training_index, target].dropna().index\n",
    "        )  # make sure we only train on rows which have this target\n",
    "        train_model.fit(\n",
    "            all_data.loc[target_train_index, features],\n",
    "            all_data.loc[target_train_index, target],\n",
    "        )  # in case targets are missing data\n",
    "        print(f\"saving new model: {train_data_model_name}\")\n",
    "        save_model(train_model, train_data_model_name)\n",
    "\n",
    "    # predict on validation data\n",
    "    all_data.loc[validation_index, prediction_col] = train_model.predict(\n",
    "        all_data.loc[validation_index, features]\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    # do the same thing for all data (for predicting on live)\n",
    "    all_data_model_name = f\"all_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{all_data_model_name}'\")\n",
    "    all_data_model = load_model(all_data_model_name)\n",
    "    if not all_data_model:\n",
    "        print(f\"model not found, creating new one\")\n",
    "        all_data_model = Ridge(alpha=0.9)\n",
    "        all_data_target_index = (\n",
    "            all_data.loc[all_index, target].dropna().index\n",
    "        )  # make sure we only train on rows which have this target\n",
    "        # train and save the model so we don't have to train next time\n",
    "        all_data_model.fit(\n",
    "            all_data.loc[all_data_target_index, features],\n",
    "            all_data.loc[all_data_target_index, target],\n",
    "        )\n",
    "        print(f\"saving new model: {all_data_model_name}\")\n",
    "        save_model(all_data_model, all_data_model_name)\n",
    "\n",
    "    # predict on live data\n",
    "    live_data[prediction_col] = all_data_model.predict(\n",
    "        live_data[features].fillna(np.nan)\n",
    "    )  # filling live data with nans makes us ignore those features if necessary\n",
    "    gc.collect()\n",
    "\n",
    "    prediction_cols.append(prediction_col)\n",
    "\n",
    "# make an ensemble\n",
    "all_data.loc[:, \"equal_weight\"] = all_data[prediction_cols].mean(axis=1)\n",
    "live_data[\"equal_weight\"] = live_data[prediction_cols].mean(axis=1)\n",
    "\n",
    "prediction_cols.append(\"equal_weight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T04:26:22.531075Z",
     "iopub.status.busy": "2023-03-27T04:26:22.530682Z",
     "iopub.status.idle": "2023-03-27T04:26:22.537723Z",
     "shell.execute_reply": "2023-03-27T04:26:22.536954Z",
     "shell.execute_reply.started": "2023-03-27T04:26:22.531050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_litigant_unsizable_rhebok', 'feature_inconsiderate_unbooted_ricer', 'feature_inured_conservable_forcer', 'feature_glibber_deficient_jakarta', 'feature_ruptured_designing_interpolator', 'feature_rose_buttoned_dandy', 'feature_unpressed_mahratta_dah', 'feature_approximal_telautographic_sharkskin', 'feature_fribble_gusseted_stickjaw', 'feature_spec_subversive_plotter', 'feature_integrative_reviviscent_governed', 'feature_tamil_grungy_empathy', 'feature_plumular_constantinian_repositing', 'feature_serpentiform_trinary_imponderability', 'feature_flawed_demonological_toady', 'feature_pruinose_raploch_roubaix', 'feature_hunchbacked_unturning_meditation', 'feature_circumnavigable_naughty_retranslation', 'feature_testicular_slashed_ventosity', 'feature_unswaddled_inenarrable_goody', 'feature_subinfeudatory_brainy_carmel', 'feature_simpatico_cadential_pup', 'feature_esculent_erotic_epoxy', 'feature_gleaming_monosyllabic_scrod', 'feature_unyielding_dismal_divertissement', 'feature_phyletic_separate_genuflexion', 'feature_geostrophic_adaptative_karla', 'feature_confusable_pursy_plosion', 'feature_unnameable_abysmal_net', 'feature_completive_pedantical_sinecurist', 'feature_dissident_templed_shippon', 'feature_guerrilla_arrested_flavine', 'feature_unrecognisable_waxier_paging', 'feature_scissile_dejected_kainite', 'feature_overjoyed_undriven_sauna', 'feature_incertain_catchable_zibet', 'feature_ruthenic_peremptory_truth', 'feature_subfusc_furriest_nervule', 'feature_inhospitable_necked_duckbill', 'feature_untired_flighty_tungstate', 'feature_adjustable_ruffled_lumberjacket', 'feature_toadyish_illiterate_famishment', 'feature_fascial_biserrate_pout', 'feature_kingly_gemmological_electrodynamometer', 'feature_applausive_forgettable_mishanter', 'feature_exhilarative_agleam_hebron', 'feature_geomorphological_uncompanioned_colander', 'feature_deflective_demographic_saragossa', 'feature_teratogenic_wet_calkin', 'feature_perpetuable_stuffed_manxman', 'feature_incognita_cleaned_asphyxiant', 'feature_gory_medullated_waverer', 'feature_precursory_maltese_wetting', 'feature_peeled_singing_smacking', 'feature_sophomoric_perseverant_sclaff', 'feature_regrettable_liberating_crabber', 'feature_revitalizing_intersectional_dysthymia', 'feature_unchaste_raisable_tetraploidy', 'feature_disapproving_behind_dampener', 'feature_maigre_twinkling_overstand', 'feature_handled_crescent_ciselure', 'feature_pacifist_unmeaning_haslet', 'feature_supportive_sublime_phenolic', 'feature_revolutionist_proportionate_headscarf', 'feature_anticivic_blistery_knot', 'feature_unsatisfactory_lovelorn_brainstorm', 'feature_spiroid_semblable_glimpse', 'feature_metagrobolized_incorrigible_berkelium', 'feature_marled_coliform_proctorship', 'feature_woodier_slimmest_supplanter', 'feature_anencephalic_unattempted_pschent', 'feature_bronchitic_miscible_inwall', 'feature_sophistic_translucid_abutment', 'feature_uninquiring_unspiritualized_gradualism', 'feature_multipolar_syncopated_ambrotype', 'feature_sunset_slouchy_alisma', 'feature_subglobose_sleekier_calcaneum', 'feature_addressable_intransitive_reconnoitrer', 'feature_flamier_confusing_dithering', 'feature_ugrian_schizocarpic_skulk', 'feature_smuggest_galvanic_memorial', 'feature_indefeasible_abject_faucet', 'feature_unpainted_censual_pinacoid', 'feature_chorionic_coated_undergraduette', 'feature_tinkly_driftiest_maurya', 'feature_pulsing_ionian_flatterer', 'feature_tragical_rainbowy_seafarer', 'feature_interpenetrative_boustrophedon_proudhon', 'feature_undisguised_photoelectric_floorboard', 'feature_sodding_choosy_eruption', 'feature_standardized_rosiny_suslik', 'feature_depleted_omnidirectional_baluchi', 'feature_unsocial_pouring_barbe', 'feature_like_inflorescent_sierra', 'feature_harmonized_intrinsic_disapproval', 'feature_kinematical_absorbable_specialisation', 'feature_disincentive_unchallenged_aerobe', 'feature_intrusive_sniffy_gangland', 'feature_pedicular_fanfold_beseecher', 'feature_tardigrade_intercommunal_propitiatory', 'feature_unwomanly_pitying_saffian', 'feature_aeriform_discomposed_moolvie', 'feature_exploding_delectable_aril', 'feature_hemihedral_fumed_marquisette', 'feature_disillusive_saltant_placidity', 'feature_cauline_herpetic_videocassette', 'feature_massive_demisable_spouse', 'feature_ugly_admissible_firm', 'feature_unventilated_sollar_bason', 'feature_nymphomaniac_cleaned_museologist', 'feature_immovable_liquified_potpie', 'feature_utmost_excitable_emir', 'feature_imitable_unnatural_samuel', 'feature_fungible_allotted_deterioration', 'feature_saut_shalwar_culpability', 'feature_dental_stormier_chape', 'feature_alive_romansh_stinging', 'feature_thermophile_noisette_swamper', 'feature_satisfied_aymaran_enterotomy', 'feature_tiptoe_decadent_statue', 'feature_undrilled_wheezier_countermand', 'feature_expended_evitable_darwinian', 'feature_corporatist_seborrheic_hopi', 'feature_undisguised_unenviable_stamen', 'feature_fearsome_merry_bluewing', 'feature_shrinelike_unreplaceable_nitrogenization', 'feature_septuple_bonapartean_sanbenito', 'feature_tottery_unmetalled_codder', 'feature_tachygraphical_sedimentological_mesoderm', 'feature_adsorbed_blizzardy_burlesque', 'feature_productile_auriform_fil', 'feature_incorporating_abominable_daily', 'feature_herbaged_brownish_consubstantialist', 'feature_solemn_wordier_needlework', 'feature_impetratory_shuttered_chewer', 'feature_referenced_biliteral_chiropody', 'feature_malpighian_vaporized_biogen', 'feature_percipient_atelectatic_cinnamon', 'feature_gobony_premonitory_twinkler', 'feature_noctilucent_subcortical_proportionality', 'feature_guardian_frore_rolling', 'feature_denuded_typed_wattmeter', 'feature_enzymatic_poorest_advocaat', 'feature_distressed_bloated_disquietude', 'feature_leaky_overloaded_rhodium', 'feature_donsie_folkish_renitency', 'feature_agee_sold_microhabitat', 'feature_basophil_urdy_matzo', 'feature_hardback_saturnalian_cyclometer', 'feature_mythic_florentine_psammite', 'feature_optic_mycelial_whimper', 'feature_coactive_bandoleered_trogon', 'feature_revitalizing_rutilant_swastika', 'feature_unappreciated_humiliated_misapprehension', 'feature_idled_unwieldy_improvement', 'feature_carbuncled_athanasian_ampul', 'feature_synodal_feisty_weave', 'feature_stannic_peevish_idocrase', 'feature_biobibliographical_carnal_atomisation', 'feature_depletory_cannular_automatism', 'feature_figurative_uncertificated_indigent', 'feature_doctrinal_viewier_dentary', 'feature_careworn_motivational_requisite', 'feature_precursory_catching_inertia', 'feature_datival_crucial_chevrotain', 'feature_just_flavescent_draff', 'feature_cephalopod_arrased_jird', 'feature_rheumatic_bravest_pantisocracy', 'feature_store_comforted_goiter', 'feature_crushed_gleg_reintroduction', 'feature_scald_vanishing_enchainment', 'feature_palimpsest_inoffensive_coiffeuse', 'feature_adored_empyreal_revel', 'feature_archaean_unregarded_caravel', 'feature_concealed_artful_thaw', 'feature_bluff_carbonyl_verbena', 'feature_falsifiable_performative_maxixe', 'feature_craggier_windier_apologia', 'feature_elaborate_intimate_bor', 'feature_meteorological_tushed_ester', 'feature_iffy_pretty_gumming', 'feature_bellicose_lunatic_glorification', 'feature_undebauched_cobaltic_guerrilla', 'feature_dysgenic_putrefied_nosegay', 'feature_mensal_amusive_phosphorylase', 'feature_herpetologic_unjoyful_lodgepole', 'feature_cyclopedic_maestoso_daguerreotypist', 'feature_epigraphic_leucocratic_rutherford', 'feature_hatched_myriad_biogen', 'feature_unspoilt_astronomical_lumper', 'feature_sovietism_interred_toile', 'feature_loftier_sightly_lyric', 'feature_unlucky_hammered_pard', 'feature_khedival_viewable_bloodlust', 'feature_milkier_gassy_pincushion', 'feature_exoergic_zoomorphic_burin', 'feature_setose_processed_crevice', 'feature_comprisable_commensurable_cyrenaic', 'feature_associate_unproper_gridder', 'feature_lowery_transcribed_muffin', 'feature_profaned_exothermal_orczy', 'feature_bursarial_southmost_kaduna', 'feature_elaborate_burning_drunkard', 'feature_pardonable_ungraceful_bedazzlement', 'feature_unholy_residential_anabaptism', 'feature_uremic_trussed_grater', 'feature_shrinelike_introverted_eagre', 'feature_predominant_unmown_concealing', 'feature_violated_telic_tuning', 'feature_directive_bioplasmic_skua', 'feature_disparate_acellular_pictish', 'feature_trimestrial_unsuspecting_guadeloupe', 'feature_epinastic_sycophantical_satinwood', 'feature_gaga_clinched_islamization', 'feature_underdeveloped_incomprehensible_traveller', 'feature_nasofrontal_hornier_sterigma', 'feature_apprentice_acheulian_extractability', 'feature_gandhian_discretional_cricoid', 'feature_nonagenarian_roundish_publication', 'feature_togate_unbailable_door', 'feature_true_legendary_shote', 'feature_normal_urochordal_proffer', 'feature_unpoisoned_migratory_uri', 'feature_chatty_circumambient_patripassian', 'feature_goyish_riparian_recipient', 'feature_intramuscular_nummulitic_wildcatter', 'feature_diatonic_duplex_bunny', 'feature_penned_insufficient_cartel', 'feature_urodele_miffier_chagall', 'feature_lower_legalism_stane', 'feature_unpreached_pickiest_lint', 'feature_ablest_mauritanian_elding', 'feature_sliced_cuneal_anouilh', 'feature_bifocal_disposable_clacton', 'feature_splashier_conservant_ultramarine', 'feature_fourieristic_allied_mugwumpery', 'feature_headiest_unguessed_religion', 'feature_nonnegotiable_errant_soya', \"feature_substantiated_denatured_hadn't\", 'feature_optical_kempt_aisle', 'feature_terroristic_tripersonal_pashm', 'feature_herniated_exasperate_victorian', 'feature_domanial_shellproof_rationing', 'feature_wetter_unbaffled_loma', 'feature_unconjugal_chiropodial_amorosity', 'feature_third_discreet_solute', 'feature_unbarking_apolitical_hibernian', 'feature_encysted_conventionalized_dematerialization', 'feature_improvable_waniest_lesson', 'feature_supererogatory_unleisured_kitling', 'feature_preceding_perturbing_radii', 'feature_closing_branchy_kirman']\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-27T00:22:20.355486Z",
     "iopub.status.idle": "2023-03-27T00:22:20.355804Z",
     "shell.execute_reply": "2023-03-27T00:22:20.355681Z",
     "shell.execute_reply.started": "2023-03-27T00:22:20.355667Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make a 50% feature neutral variation of the ensemble model\n",
    "all_data[\"half_neutral_equal_weight\"] = neutralize(\n",
    "    df=all_data.loc[validation_index, :],\n",
    "    columns=[f\"equal_weight\"],\n",
    "    neutralizers=features,\n",
    "    proportion=0.5,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL,\n",
    "    verbose=True,\n",
    ")\n",
    "# do the same for live data\n",
    "live_data[\"half_neutral_equal_weight\"] = neutralize(\n",
    "    df=live_data,\n",
    "    columns=[f\"equal_weight\"],\n",
    "    neutralizers=features,\n",
    "    proportion=0.5,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "prediction_cols.append(\"half_neutral_equal_weight\")\n",
    "\n",
    "model_to_submit = f\"half_neutral_equal_weight\"\n",
    "\n",
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "all_data.loc[validation_index, \"prediction\"] = all_data.loc[\n",
    "    validation_index, model_to_submit\n",
    "].rank(pct=True)\n",
    "live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
    "all_data.loc[validation_index, \"prediction\"].to_csv(\n",
    "    f\"validation_predictions_{current_round}.csv\"\n",
    ")\n",
    "live_data[\"prediction\"].to_csv(f\"live_predictions_{current_round}.csv\")\n",
    "\n",
    "validation_example_preds = pd.read_parquet(\n",
    "    f\"{dataset_name}/validation_example_preds.parquet\"\n",
    ")\n",
    "all_data.loc[validation_index, EXAMPLE_PREDS_COL] = validation_example_preds[\n",
    "    \"prediction\"\n",
    "]\n",
    "\n",
    "# get some stats about each of our models to compare...\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "validation_stats = validation_metrics(\n",
    "    all_data.loc[validation_index, :],\n",
    "    prediction_cols,\n",
    "    example_col=EXAMPLE_PREDS_COL,\n",
    "    fast_mode=True,\n",
    "    target_col=TARGET_COL,\n",
    ")\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Done! Next steps:\n",
    "    1. Go to numer.ai/tournament (make sure you have an account)\n",
    "    2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n",
    "    3. Submit tournament_predictions_{current_round}.csv to the \"Upload Predictions\" button\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
